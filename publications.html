<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Qing Zhang</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

				<!-- Global site tag (gtag.js) - Google Analytics -->
				<script async src="https://www.googletagmanager.com/gtag/js?id=G-QNEBKBS3KZ"></script>
				<script>
				window.dataLayer = window.dataLayer || [];
				function gtag(){dataLayer.push(arguments);}
				gtag('js', new Date());
	
				gtag('config', 'G-QNEBKBS3KZ');
				</script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Qing Zhang</a>
					
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">About Me</a></li>
							<!-- <li><a href="aboutme.html">About Me</a></li> -->
							<li><a href="news.html">News</a></li>
							<li class="active"><a href="publications.html">Publications</a></li>
							<li><a href="drawings.html">Drawings</a></li>
							<!-- <li><a href="artworks.html">Artworks</a></li> -->
							<li><a href="Photography.html">Photography</a></li>
							<li><a href="cv.html">CV</a></li>
						</ul>
						<ul class="icons">
							<!-- <li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li> -->
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Post -->
						<section class="post">
							<header class="major">
								
							
								<span class="date">Jun 02, 2024  updated</span>
								<!-- <p style="text-align: left;">A full paper and a demo will be published by UIST'22 soon.<br> -->
									A full list of publications can be found on <a href="pdfs/inner-self-drawing-machine.pdf">Google Scholar</a>.
								</p>
								<!-- <h1>This is a<br />
								Generic Page</h1> -->
								<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
								facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
								amet nullam sed etiam veroeros.</p> -->
							</header>
							<div class="image main"><img src="images/InnerSelfMachine.jpg" alt="" /></div>
							<h3>Inner self drawing machine
							</h3>
							<p>
								<b>Qing Zhang</b>, Fan Xie, Yifei Huang, Yun Suen Pai, George Chernyshov, Jing Huang, Xiongqi Wang, Jamie A Ward, Kai Kunze.
							</br>
								<a href="https://sa2022.siggraph.org/en/">SIGGRAPH Asia'23, Art Gallery</a></br>
							</br>
							Besides men and women, people can be neither man nor woman, fluid identity, transgender, and agender. 
							Not only that, in terms of sexual orientation, besides heterosexuals, there are homosexuals, bisexuals, 
							pansexuals, and asexuals as well. However, ignorance and lacking empathetic understanding of those sexual 
							minorities make their lives harsh and suffering. For instance, in the case of transgender people, 28% of 
							them postponed their health care due to discrimination, 19% of them refused medical care altogether, and 
							28% of them experienced verbal harassment by medical professionals, according to a 2011 national transgender 
							discrimination survey (USA). Unluckily, we are apt to generate a basic understanding of others based on their 
							gender expressions and use such irresponsible and heuristic findings to deal with others. Thus, we decided to 
							create an installation to at least minimize the gap for a moment when the audience can enjoy themselves by 
							watching the drawing performance of their ideal portrait (inner-self).
							Regarding the AI portrait painter, we leverage StyleGAN to generate the continuous gender spectrum of each participant based on their facial 
							features, in which they can choose their ideal gender representation that reflects their inner self the most. Then our AI portrait painter 
							"draws" the selected "self" on the canvas. In general professional painters can detect and draw the most confident and beautiful us; on the other hand, we tend to exaggerate our flaws and ignore our attractive parts. When the drawing performance finishes, the audience can receive the drawing result as a well-printed portrait simultaneously. The printed portrait also works as a souvenir of participating in our exhibition. Our work aims to raise an empathic understanding </br>
							https://dl.acm.org/doi/10.1145/3550470.3558429 </br>    
								<a href="pdfs/inner-self-drawing-machine.pdf">Download this paper</a>
							</p>
							<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
							<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
						</section>
						<!-- Post -->
						<section class="post">
							<header class="major">
								<span class="date">April 20,  updated</span>
								<!-- <p style="text-align: left;">A full paper and a demo will be published by UIST'22 soon.<br> -->
								
								</p>
								<!-- <h1>This is a<br />
								Generic Page</h1> -->
								<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
								facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
								amet nullam sed etiam veroeros.</p> -->
							</header>
							<div class="image main"><img src="images/conceptDescriptiveFig.png" alt="" /></div>
							<h3>First Bite/Chew: distinguish diferent types of food by first biting/chewing and the corresponding hand movement
							</h3>
							<p>
								Junyu Chen, Xiongqi Wang, Juling Li, Thad Starner, George Chernyshov, Jing Huang, Yifei Huang, Kai Kunze, <b>Qing Zhang</b>
							</br>
								<a href="https://chi2023.acm.org/">CHI'23, Late-breaking work</a></br>
							</br>
							Imbalanced food intake contributes to various diseases, 
							such as obesity, diabetes, high blood pressure, high cholesterol, 
							heart disease, and type-2 diabetes. At the same time, 
							food intake monitoring systems play a signifcant role in the treatment. 
							Most current food intake tracking methods are camera-based, 
							on-body sensor-based, microphone based, and self-reported. 
							The challenges that remain are social acceptance, lightweight, 
							easy to use, and inexpensive. Our method leverages two 
							6-axe Inertial Measurement Units (IMU) on the glasses’ 
							leg and the wrist to detect the user’s food intake activities 
							using a machine learning capable Micro Controller Unit (MCU). 
							We introduced the concept of the frst bite/chew, which is a 
							stable and reliable indicator to distinguish food types. Our 
							implementation results show that our method can distinguish 
							seven kinds of food at an accuracy of 93.26% (average) over 
							all four participants.</br>
							https://dl.acm.org/doi/10.1145/3582700.3583708 </br>    
								<a href="pdfs/firstBite.pdf">Download this paper</a>
							</p>
							<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
							<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
						</section>



						<section class="post">
							<header class="major">
								<span class="date">March 16,  updated</span>
								<!-- <p style="text-align: left;">A full paper and a demo will be published by UIST'22 soon.<br> -->
								
								</p>
								<!-- <h1>This is a<br />
								Generic Page</h1> -->
								<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
								facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
								amet nullam sed etiam veroeros.</p> -->
							</header>
							<div class="image main"><img src="images/AHsTeaserNew.png" alt="" /></div>
							<h3>First Bite/Chew: distinguish typical allergic food by two IMUs
							</h3>
							<p>
								Juling Li, Xiongqi Wang, Junyu Chen, Thad Starner, George Chernyshov, Jing Huang, Kai Kunze, <b>Qing Zhang</b>
							</br>
								<a href="https://augmented-humans.org/">AHs'23, Best Poster Award</a></br>
							</br>
							Eating or overtaking allergic foods may cause fatal symptoms or even death for 
							people with food allergies. Most current food intake tracking methods are camera-based, 
							on-body sensor-based, microphone based, and self-reported. However, 
							challenges that remain are allergic food detection, social acceptance, lightweight, 
							easy to use, and inexpensive. 
    
							Our approach leverages the first bite/chew and the corresponding hand movement as an 
							indicator to distinguish typical types of the allergic food. Our initial feasibility 
							study shows that our approach can distinguish six types of food at an accuracy of 89.7% over 
							all four participants' mixed data. Particularly, our method successfully detected and 
							distinguished typical allergic foods such as burgers (wheat), instant noodles (wheat), 
							peanuts, egg fried rice, and edamame, which can be expected to contribute to not only 
							personal use but also medical usage.</br>
							https://dl.acm.org/doi/10.1145/3582700.3583708 </br>    
								<a href="pdfs/3582700.3583708.pdf">Download this paper</a>
							</p>
							<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
							<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
						</section>

						<section class="post">
							<header class="major">
								<span class="date">October 30, 2022 updated</span>
								<!-- <p style="text-align: left;">A full paper and a demo will be published by UIST'22 soon.<br> -->
								
								</p>
								<!-- <h1>This is a<br />
								Generic Page</h1> -->
								<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
								facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
								amet nullam sed etiam veroeros.</p> -->
							</header>
							<div class="image main"><img src="images/uistpaperteaser.png" alt="" /></div>
							<h3>Seeing our Blind Spots: Smart Glasses-based Simulation to Increase Design Students' Awareness of Visual Impairment
							</h3>
							<p>
								<b>Qing Zhang</b>, Giulia Barbareschi, Yifei Huang, Juling Li, Yun Suen Pai, Jamie Ward, Kai Kunze. 
								
								<a href="https://uist.acm.org/uist2022/">UIST'22</a></br>
							</br>
								MAs the population ages, many will acquire visual impairments. 
								To improve design for these users, it is essential to build awareness of their perspective during everyday routines, 
								especially for design students. Although several visual impairment simulation toolkits exist in both academia and as commercial products, 
								analog, and static visual impairment simulation tools do not simulate effects concerning the user’s eye movements. Meanwhile, 
								VR and video see-through- based AR simulation methods are constrained by smaller fields of view when compared with the natural 
								human visual field and also suffer from vergence-accommodation conflict (VAC) which correlates with visual fatigue, headache, 
								and dizziness. In this paper, we enable an on-the-go, VAC-free, visually impaired experience by leveraging our optical see-through 
								glasses. The FOV of our glasses is approximately 160 degrees for horizontal and 140 degrees for vertical, and participants can 
								experience both losses of central vision and loss of peripheral vision at different severities. Our evaluation (n =14) indicates that 
								the glasses can significantly and effectively reduce visual acuity and visual field without causing typical motion sickness symptoms 
								such as headaches and or visual fatigue. Questionnaires and qualitative feedback also showed how the glasses helped to increase 
								participants’ awareness of visual impairment.</br>
								https://doi.org/10.1145/3526113.3545687 </br>    
								<a href="pdfs/uist22paper.pdf">Download this paper</a>
							</p>
							<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
							<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
						</section>

						
						
							<section class="post">
								<header class="major">
									<span class="date">October 19, 2022 updated</span>
									<!-- <p style="text-align: left;">A full paper and a demo will be published by UIST'22 soon.<br> -->
									
									</p>
									<!-- <h1>This is a<br />
									Generic Page</h1> -->
									<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
									facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
									amet nullam sed etiam veroeros.</p> -->
								</header>
								<div class="image main"><img src="images/ISWC.jpg" alt="" /></div>
								<h3>Tunnel Vision – Dynamic Peripheral Vision Blocking Glasses forReducing Motion Sickness Symptoms 
								</h3>
								<p>
									<b>Qing Zhang</b>, Hiroo Yamamura, Holger Baldauf, Dingding Zheng, Kanyu Chen, Junichi Yamaoka,Kai Kunze. <a href="https://www.iswc.net/iswc21/">ISWC'21</a></br>
								</br>
									Motion sickness affects roughly a third of all people. Narrowing thefield of view (FOV) can help to reduce motion sickness symptoms.
									In this paper, we present Tunnel Vision, a type of smart glassesthat can dynamically block a wearer’s 
									peripheral vision area usingswitchable polymer dispersed liquid crystal (PDLC) film. We evalu-ate the 
									prototype in a virtual reality environment. Our experiments(n=19) suggest that Tunnel Vision statistically 
									significantly reducesthe following Simulator Sickness Questionnaire (SSQ) related mo-tion sickness symptoms 
									without impacting immersion: "difficultyconcentrating" (F(2,35) = 4.121, p = 0.025), "head feeling heavy"(F(2,35) 
									= 3.231, p = 0.051) and "nausea" (F(2,35) = 3.145, p = 0.055). (PDF) Tunnel Vision – Dynamic Peripheral Vision Blocking Glasses for 
									Reducing Motion Sickness Symptoms. </br>
									https://doi.org/10.1145/3460421.3478824 </br>    
									<a href="pdfs/3460421.3478824.pdf">Download this paper</a>
								</p>
								<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
								<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
							</section>

							<section class="post">
								<header class="major">
									<span class="date">June 12, 2022 updated</span>
									<!-- <h1>This is a<br />
									Generic Page</h1> -->
									<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
									facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
									amet nullam sed etiam veroeros.</p> -->
								</header>
								<div class="image main"><img src="images/GazeSYNCteaser.png" alt="" /></div>
								<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
								<h3>Programmable Peripheral Vision: augment/reshape human visual perception
								</h3>
								<p>
									<b>Qing Zhang</b>, Yifei Huang, George Chernyshov, Juling Li, Yun Suen Pai, and Kai Kunze. <a href="https://iui.acm.org/2022/index.html">IUI'22</a></br>
								</br>
									Motion sickness affects roughly a third of all people. Narrowing thefield of view (FOV) can help to reduce motion sickness symptoms.
									In this paper, we present Tunnel Vision, a type of smart glassesthat can dynamically block a wearer’s 
									peripheral vision area usingswitchable polymer dispersed liquid crystal (PDLC) film. We evalu-ate the 
									prototype in a virtual reality environment. Our experiments(n=19) suggest that Tunnel Vision statistically 
									significantly reducesthe following Simulator Sickness Questionnaire (SSQ) related mo-tion sickness symptoms 
									without impacting immersion: "difficultyconcentrating" (F(2,35) = 4.121, p = 0.025), "head feeling heavy"(F(2,35) 
									= 3.231, p = 0.051) and "nausea" (F(2,35) = 3.145, p = 0.055). (PDF) Tunnel Vision – Dynamic Peripheral Vision Blocking Glasses for 
									Reducing Motion Sickness Symptoms. 
									</br>
									https://doi.org/10.1145/3490100.3516469</br>    
									<a href="pdfs/3491101.3503821.pdf">Download this paper</a>
								</p>
								
								
								
								
								<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
							</section>

							<section class="post">
								<header class="major">
									<span class="date">June 12, 2022 updated</span>
									<!-- <h1>This is a<br />
									Generic Page</h1> -->
									<!-- <p>Aenean ornare velit lacus varius enim ullamcorper proin aliquam<br />
									facilisis ante sed etiam magna interdum congue. Lorem ipsum dolor<br />
									amet nullam sed etiam veroeros.</p> -->
								</header>
								<div class="image main"><img src="images/3glasses.png" alt="" /></div>
								<h3>GazeSync: Eye Movement Transfer Using an Optical Eye Tracker and Monochrome Liquid Crystal Displays 
								</h3>
								<p>
									<b>Qing Zhang. </b><a href="https://chi2022.acm.org/">CHI'22</a></br>
								</br>
									Motion sickness affects roughly a third of all people. Narrowing thefield of view (FOV) can help to reduce motion sickness symptoms.
									In this paper, we present Tunnel Vision, a type of smart glassesthat can dynamically block a wearer’s 
									peripheral vision area usingswitchable polymer dispersed liquid crystal (PDLC) film. We evalu-ate the 
									prototype in a virtual reality environment. Our experiments(n=19) suggest that Tunnel Vision statistically 
									significantly reducesthe following Simulator Sickness Questionnaire (SSQ) related mo-tion sickness symptoms 
									without impacting immersion: "difficultyconcentrating" (F(2,35) = 4.121, p = 0.025), "head feeling heavy"(F(2,35) 
									= 3.231, p = 0.051) and "nausea" (F(2,35) = 3.145, p = 0.055). (PDF) Tunnel Vision – Dynamic Peripheral Vision Blocking Glasses for 
									Reducing Motion Sickness Symptoms. </br>
									https://doi.org/10.1145/3491101.3503821</br>    
									<a href="pdfs/3490100.3516469.pdf">Download this paper</a>
								</p>
								<!-- <p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Curabitur sapien risus, commodo eget turpis at, elementum convallis enim turpis, lorem ipsum dolor sit amet nullam.</p> -->
								<!-- <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. Pellentesque aliquam maximus risus. Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum.</p> -->
							</section>
					</div>

				<!-- Footer -->
				<footer id="footer">
					<section>
						<form 
							action="https://formspree.io/f/mbjwbzgk"
  							method="POST"
						>
							<div class="fields">
								<div class="field">
									<label for="name">Name</label>
									<input type="text" name="name" id="name" />
								</div>
								<div class="field">
									<label for="email">Email</label>
									<input type="text" name="email" id="email" />
								</div>
								<div class="field">
									<label for="message">Message</label>
									<textarea name="message" id="message" rows="3"></textarea>
								</div>
							</div>
							<ul class="actions">
								<li><input type="submit" value="Send Message" /></li>
							</ul>
						</form>
					</section>
					<section class="split contact">
						<!-- <section class="alt">
							<h3>Address</h3>
							<p>1234 Somewhere Road #87257<br />
							Nashville, TN 00000-0000</p>
						</section>
						<section>
							<h3>Phone</h3>
							<p><a href="#">(000) 000-0000</a></p>
						</section> -->
						<section>
							<h3>Email</h3>
							<p><a href="#">qzkiyoshi at gmail dot com</a></p>
						</section>
						<section>
							<!-- <h3>Social</h3> -->
							<!-- <ul class="icons alt"> -->
								<!-- <li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li> -->
							</ul>
						</section>
					</section>
				</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>